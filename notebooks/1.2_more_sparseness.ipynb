{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Downloading dataset\n"
   ],
   "metadata": {
    "id": "V6rMe5uxdUuZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "def download_and_unzip(url, extract_to='.'):\n",
    "    http_response = urlopen(url)\n",
    "    zipfile = ZipFile(BytesIO(http_response.read()))\n",
    "    zipfile.extractall(path=extract_to)\n",
    "\n",
    "\n",
    "download_and_unzip(\"https://files.grouplens.org/datasets/movielens/ml-100k.zip\",\n",
    "                   extract_to='.')"
   ],
   "metadata": {
    "id": "Ss90qQLfdTPq"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_LPed5cuAEa"
   },
   "source": [
    "Next, let's import all of the modules that we'll use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y9fonQcxt3do"
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn import ensemble as ens\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bulding training and testing datasets\n",
    "\n",
    "I will use already splitted train and test datasets - ua.base, ua.test"
   ],
   "metadata": {
    "id": "BvAGcOMgdcCN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g0Nuptl1RATp"
   },
   "outputs": [],
   "source": [
    "def make_dfs():\n",
    "  columns_name=['user_id','item_id','rating','timestamp']\n",
    "  train_df = pd.read_csv(\"./ml-100k/ua.base\",sep=\"\\t\",names=columns_name)\n",
    "  test_df = pd.read_csv(\"./ml-100k/ua.test\",sep=\"\\t\",names=columns_name)\n",
    "\n",
    "  return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "diJHiO6nSImi"
   },
   "outputs": [],
   "source": [
    "def preproc(train_df, test_df):\n",
    "    film_columns = [\"item_id\", \"movie title\", \"release date\", \"video release date\",\n",
    "              \"IMDb URL\", \"unknown\", \"Action\", \"Adventure\", \"Animation\",\n",
    "              \"Children's\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\",\n",
    "              \"Film-Noir\", \"Horror\", \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\",\n",
    "              \"Thriller\", \"War\", \"Western\"]\n",
    "\n",
    "    # Reading and preprocessing data about movies\n",
    "    films_df = pd.read_csv(\"./ml-100k/u.item\", sep=\"|\", names=film_columns, encoding='latin-1')\n",
    "    films_df.drop([\"movie title\", \"release date\", \"IMDb URL\", \"unknown\", \"video release date\"], axis = 1, inplace = True)\n",
    "\n",
    "    # Merging info about movies to datasets\n",
    "    train_df = pd.merge(train_df, films_df, how='left', left_on='item_id', right_on='item_id')\n",
    "    test_df = pd.merge(test_df, films_df, how='left', left_on='item_id', right_on='item_id')\n",
    "\n",
    "    # Reading and preprocessing data about users\n",
    "    user_columns = [\"user_id\", \"age\", \"sex\", \"occupation\", \"zip_code\"]\n",
    "    user_df = pd.read_csv(\"./ml-100k/u.user\", sep=\"|\", names=user_columns, encoding='latin-1')\n",
    "    user_df[\"sex\"] = pp.LabelEncoder().fit_transform(user_df[\"sex\"])\n",
    "    occup_df = pd.read_csv(\"./ml-100k/u.occupation\", sep=\"\\t\", names=[\"jobs\"])\n",
    "    le = pp.LabelEncoder()\n",
    "    le.fit(occup_df[\"jobs\"])\n",
    "    user_df[\"occupation\"] = le.transform(user_df[\"occupation\"])\n",
    "    user_df.drop([\"zip_code\"], axis = 1, inplace = True)\n",
    "\n",
    "    # Merging info about users to datasets\n",
    "    train_df = pd.merge(train_df, user_df, how='left', left_on='user_id', right_on='user_id')\n",
    "    test_df = pd.merge(test_df, user_df, how='left', left_on='user_id', right_on='user_id')\n",
    "\n",
    "    # Preprocessing and normalising some fields\n",
    "    train_df[\"rating\"]  = train_df[\"rating\"] - 1\n",
    "    train_df[\"age\"] = train_df[\"age\"] / user_df[\"age\"].abs().max()\n",
    "\n",
    "    # Sparsing occupation info\n",
    "    encoder = pp.OneHotEncoder(sparse_output=False)\n",
    "    encoder.fit(train_df[[\"occupation\"]])\n",
    "    new_feat = encoder.transform(train_df[[\"occupation\"]])\n",
    "    new_cols = pd.DataFrame(new_feat, columns=encoder.get_feature_names_out([\"occupation\"]))\n",
    "    train_df = pd.concat([train_df, new_cols], axis=1)\n",
    "    train_df.drop(\"occupation\", axis=1, inplace=True)\n",
    "\n",
    "    new_feat = encoder.transform(test_df[[\"occupation\"]])\n",
    "    new_cols = pd.DataFrame(new_feat, columns=encoder.get_feature_names_out([\"occupation\"]))\n",
    "    test_df = pd.concat([test_df, new_cols], axis=1)\n",
    "    test_df.drop(\"occupation\", axis=1, inplace=True)\n",
    "\n",
    "    test_df[\"rating\"]  = test_df[\"rating\"] - 1\n",
    "    test_df[\"age\"] = test_df[\"age\"] / user_df[\"age\"].abs().max()\n",
    "\n",
    "    # Getting X, y division\n",
    "    train_df.drop([\"item_id\", \"user_id\", \"timestamp\"], axis = 1, inplace = True)\n",
    "    train_y = train_df[\"rating\"].values\n",
    "    train_x = train_df.drop('rating', axis=1).values\n",
    "\n",
    "    test_df.drop([\"item_id\", \"user_id\", \"timestamp\"], axis = 1, inplace = True)\n",
    "    test_y = test_df[\"rating\"].values\n",
    "    test_x = test_df.drop('rating', axis=1).values\n",
    "\n",
    "    print(test_df.info())\n",
    "    return train_x, train_y, test_x, test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3Kd3_Al5ZaJM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fa82e7cc-4b26-43b4-cddd-0a1c95a978c2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9430 entries, 0 to 9429\n",
      "Data columns (total 42 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   rating         9430 non-null   int64  \n",
      " 1   Action         9430 non-null   int64  \n",
      " 2   Adventure      9430 non-null   int64  \n",
      " 3   Animation      9430 non-null   int64  \n",
      " 4   Children's     9430 non-null   int64  \n",
      " 5   Comedy         9430 non-null   int64  \n",
      " 6   Crime          9430 non-null   int64  \n",
      " 7   Documentary    9430 non-null   int64  \n",
      " 8   Drama          9430 non-null   int64  \n",
      " 9   Fantasy        9430 non-null   int64  \n",
      " 10  Film-Noir      9430 non-null   int64  \n",
      " 11  Horror         9430 non-null   int64  \n",
      " 12  Musical        9430 non-null   int64  \n",
      " 13  Mystery        9430 non-null   int64  \n",
      " 14  Romance        9430 non-null   int64  \n",
      " 15  Sci-Fi         9430 non-null   int64  \n",
      " 16  Thriller       9430 non-null   int64  \n",
      " 17  War            9430 non-null   int64  \n",
      " 18  Western        9430 non-null   int64  \n",
      " 19  age            9430 non-null   float64\n",
      " 20  sex            9430 non-null   int64  \n",
      " 21  occupation_0   9430 non-null   float64\n",
      " 22  occupation_1   9430 non-null   float64\n",
      " 23  occupation_2   9430 non-null   float64\n",
      " 24  occupation_3   9430 non-null   float64\n",
      " 25  occupation_4   9430 non-null   float64\n",
      " 26  occupation_5   9430 non-null   float64\n",
      " 27  occupation_6   9430 non-null   float64\n",
      " 28  occupation_7   9430 non-null   float64\n",
      " 29  occupation_8   9430 non-null   float64\n",
      " 30  occupation_9   9430 non-null   float64\n",
      " 31  occupation_10  9430 non-null   float64\n",
      " 32  occupation_11  9430 non-null   float64\n",
      " 33  occupation_12  9430 non-null   float64\n",
      " 34  occupation_13  9430 non-null   float64\n",
      " 35  occupation_14  9430 non-null   float64\n",
      " 36  occupation_15  9430 non-null   float64\n",
      " 37  occupation_16  9430 non-null   float64\n",
      " 38  occupation_17  9430 non-null   float64\n",
      " 39  occupation_18  9430 non-null   float64\n",
      " 40  occupation_19  9430 non-null   float64\n",
      " 41  occupation_20  9430 non-null   float64\n",
      "dtypes: float64(22), int64(20)\n",
      "memory usage: 3.1 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = make_dfs()\n",
    "train_x, train_y, test_x, test_y = preproc(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_user_lines = {} # Getting useful data for recommendation\n",
    "test_items = []\n",
    "for i, data in test_df.iterrows():\n",
    "  test_items.append(data[\"item_id\"])\n",
    "  if data[\"user_id\"] not in test_user_lines.keys():\n",
    "    test_user_lines[data[\"user_id\"]] = [i]\n",
    "  else:\n",
    "    test_user_lines[data[\"user_id\"]].append(i)"
   ],
   "metadata": {
    "id": "V3XBWJ4zxlEf"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using best pipeline I got from tpot:"
   ],
   "metadata": {
    "id": "LNC70EfmfCab"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = XGBClassifier(MultinomialNB(alpha=10.0, fit_prior=False), learning_rate=0.01, max_depth=9, min_child_weight=13, n_estimators=100, n_jobs=1, subsample=0.45, verbosity=0)\n",
    "clf.fit(train_x, train_y)\n",
    "clf.score(test_x, test_y) # Accuracy may not be high, but our main concern is not accuracy"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlVyQ5z_66s4",
    "outputId": "d168941f-08cb-41eb-ad07-758c6e8d6f61"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:726: FutureWarning: Pass `objective` as keyword args.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.3637327677624602"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating model\n",
    "As the main metric for evaluating my system I decided to use\n",
    "ndcg score. I chose it because of non-binary notions of relevance, in our case ratings."
   ],
   "metadata": {
    "id": "Gtp5MhIxfe2k"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "preds = [] # Predictions\n",
    "for i in range(len(test_y)):\n",
    "  pred = clf.predict(test_x[i, :].reshape(1, -1))\n",
    "  preds.append(pred[0])"
   ],
   "metadata": {
    "id": "FXgyD2ED9WrR"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"Mean absolute error: \",  mean_absolute_error(test_y, preds))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jkhw8EA0-UKg",
    "outputId": "3a179531-f039-498d-9954-0bd8dd45abfb"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean absolute error:  0.8632025450689289\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def find_ndcg(user_id):\n",
    "  predictions = [[clf.predict(test_x[j, :].reshape(1, -1))[0] for j in test_user_lines[user_id]]]\n",
    "  real_rating = [[test_y[j] for j in test_user_lines[user_id]]]\n",
    "\n",
    "  return ndcg_score(real_rating, predictions)"
   ],
   "metadata": {
    "id": "r9fbCMdJAzcD"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate():\n",
    "  ndcg = 0\n",
    "  total = 0\n",
    "  for i in test_user_lines.keys():\n",
    "    total += 1\n",
    "    ndcg += find_ndcg(i)\n",
    "  return ndcg / total"
   ],
   "metadata": {
    "id": "-HSicWLDy6oh"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Mean ndcg score: \", evaluate()) #  As you see, result is not bad"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "55f01c59-f69f-4af1-f2c1-0b3bd718cad9",
    "id": "lEnaIgCea5vy"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean ndcg score:  0.8975866311141109\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recommendation example:"
   ],
   "metadata": {
    "id": "JwgWE9yIfxNV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def recommend_10(user_id): # Since for testing I use ua.test, where each user got\n",
    "  # exactly 10 ratings, I will recommend user 10 movies based on my predicted ratings\n",
    "  # of movies\n",
    "  predictions = [clf.predict(test_x[j, :].reshape(1, -1))[0] for j in test_user_lines[user_id]]\n",
    "  real_rating = [test_y[j] for j in test_user_lines[user_id]]\n",
    "  recommendations = [[test_items[i]] for i in test_user_lines[user_id]]\n",
    "  for i in range(len(predictions)):\n",
    "    recommendations[i].append(predictions[i])\n",
    "  recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  ideal_recommendations = [[test_items[i]] for i in test_user_lines[user_id]]\n",
    "  for i in range(len(real_rating)):\n",
    "    ideal_recommendations[i].append(real_rating[i])\n",
    "  ideal_recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  print(\"My recommendations: \", [i[0] for i in recommendations])\n",
    "  print(\"Ideal recommendations: \", [i[0] for i in ideal_recommendations])\n",
    "\n"
   ],
   "metadata": {
    "id": "lQiUb2ocXhqI"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "recommend_10(8) # Recommend 10 movies for user #8"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bKjAXyZNLFhq",
    "outputId": "c8ddddc3-326c-4b6a-acf5-8ca29072fce8"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "My recommendations:  [22, 50, 79, 89, 182, 294, 338, 385, 457, 550]\n",
      "Ideal recommendations:  [22, 50, 182, 79, 89, 338, 294, 550, 385, 457]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# As you see, my recommendations are not far from ideal ones, based on the test data"
   ],
   "metadata": {
    "id": "axATnZIxaSkU"
   },
   "execution_count": 15,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
